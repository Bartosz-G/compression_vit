{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from preprocessing.dataset import CIFAR10_custom\n",
    "from preprocessing.transforms import CompressedToTensor, ZigZagOrder, ChooseAC, FlattenZigZag\n",
    "from torchvision.transforms import Compose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "download_path = os.path.join('data', 'cifar10')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "transform = Compose([CompressedToTensor(),\n",
    "                     ZigZagOrder(),\n",
    "                     ChooseAC(5)])\n",
    "\n",
    "\n",
    "cifar_compressed = CIFAR10_custom(root=download_path, train=True, transform=transform, target_transform=None, download = False, compression=None)\n",
    "\n",
    "cifar_compressed_test = CIFAR10_custom(root=download_path, train=False, transform=transform, target_transform=None, download = False, compression=None)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "cifar_compressed.to_ycbcr(in_place=True)\n",
    "cifar_compressed.compress(in_place=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "train_loader = DataLoader(cifar_compressed, batch_size=batch_size, shuffle=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([16, 3, 16, 6])"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(train_loader))[0].shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "net = nn.Sequential(\n",
    "    nn.Linear(6, 248),\n",
    "    nn.TransformerEncoderLayer(248, 8, dim_feedforward=1024, dropout=0)\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "img1 = cifar_compressed[0][0].unsqueeze(0)\n",
    "img2 = cifar_compressed[1][0].unsqueeze(0)\n",
    "img = torch.cat((img1, img2), 0).to(torch.float32)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 16, 6])\n",
      "torch.Size([2, 48, 6])\n"
     ]
    }
   ],
   "source": [
    "print(img.shape)\n",
    "img = img.permute(0, 2, 1, 3).reshape(2 ,-1, 6)\n",
    "print(img.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[[ 37., -13.,  -7.,   3.,  -7.,  -4.],\n         [ 50.,   3.,   3.,   0.,   1.,   1.],\n         [ 69.,  -2.,  -2.,   0.,  -1.,  -1.],\n         [ 42.,   4.,   7.,   0.,  -5.,   1.],\n         [ 49.,   0.,   0.,   1.,   1.,   0.],\n         [ 70.,   0.,   0.,   0.,  -1.,   0.],\n         [ 44.,  -2.,   8.,   0.,  -2.,   4.],\n         [ 50.,  -1.,  -1.,   1.,   1.,   0.],\n         [ 70.,   0.,   0.,   0.,  -1.,   1.],\n         [ 45.,   8.,   9.,   6.,  -5.,  -2.],\n         [ 50.,  -1.,   0.,   0.,   0.,   1.],\n         [ 70.,   1.,   0.,   0.,   0.,   0.],\n         [ 50.,  12.,   3.,  -2.,  -5.,  -1.],\n         [ 49.,  -1.,   0.,   0.,   1.,   0.],\n         [ 70.,   0.,   0.,   0.,   0.,   0.],\n         [ 55., -11., -23.,  -2.,  16.,   5.],\n         [ 48.,   0.,   1.,   2.,   0.,   1.],\n         [ 71.,   0.,   0.,  -1.,  -1.,  -1.],\n         [ 72.,   8., -36.,   3.,   2.,  -1.],\n         [ 48.,  -1.,   1.,   2.,  -1.,   0.],\n         [ 71.,   2.,   3.,  -1.,   1.,   0.],\n         [ 51.,   3.,  -5.,   2.,  -7.,   2.],\n         [ 48.,   0.,   0.,   0.,   0.,   0.],\n         [ 71.,   0.,   1.,  -1.,   0.,   0.],\n         [ 54.,  -6.,  -3.,  -5.,   5.,   8.],\n         [ 48.,  -1.,   1.,   0.,   0.,   1.],\n         [ 71.,   1.,  -1.,   1.,   0.,  -1.],\n         [ 80.,   7.,  11.,   1.,  -4.,   2.],\n         [ 49.,   1.,   1.,   0.,   0.,   1.],\n         [ 70.,  -1.,  -1.,   0.,   0.,  -1.],\n         [ 73.,  -6.,  23.,   3.,  -2.,   7.],\n         [ 49.,  -1.,   2.,   0.,   1.,   1.],\n         [ 68.,   1.,  -2.,   0.,   0.,   0.],\n         [ 58.,   5.,   2.,   4.,  12.,  13.],\n         [ 43.,   2.,   1.,  -1.,   0.,   2.],\n         [ 68.,   0.,   2.,  -1.,   0.,   0.],\n         [ 65.,   4., -12.,   2.,   1.,   5.],\n         [ 39.,  -1.,   4.,   1.,  -2.,   1.],\n         [ 73.,   1.,   0.,   0.,   0.,   0.],\n         [ 59.,   1.,   6.,  10.,  -3.,  -1.],\n         [ 47.,  -1.,   1.,   0.,   1.,   0.],\n         [ 72.,   0.,   0.,  -1.,   0.,   0.],\n         [ 51.,   4.,  -2.,  -1.,  -1.,  -2.],\n         [ 47.,   0.,   0.,   0.,   0.,   0.],\n         [ 72.,   0.,   0.,   0.,   0.,   0.],\n         [ 53.,  -3.,   8.,   1.,  -3.,  -5.],\n         [ 47.,   0.,  -3.,   0.,   0.,   1.],\n         [ 69.,   0.,   0.,   0.,   0.,   0.]],\n\n        [[ 71.,  -3.,  13., -11., -12.,   5.],\n         [ 64.,   0.,  -1.,   0.,  -1.,   0.],\n         [ 57.,   1.,   1.,   0.,   0.,   0.],\n         [ 73., -15.,  24., -13.,  -2.,   9.],\n         [ 62.,   0.,  -1.,   0.,   0.,   0.],\n         [ 58.,  -1.,   2.,   0.,   1.,   1.],\n         [ 84.,  34.,  -5.,  -3.,   5., -10.],\n         [ 59.,   1.,   0.,  -1.,   0.,   0.],\n         [ 60.,   0.,   0.,   0.,   0.,   0.],\n         [ 40.,   1.,  -1.,   0.,  -1.,  -1.],\n         [ 57.,   0.,   0.,   0.,   0.,   1.],\n         [ 60.,   0.,   0.,   0.,   0.,   0.],\n         [ 63., -14., -14.,   5.,   2.,  -3.],\n         [ 65.,   0.,   0.,  -2.,   0.,   0.],\n         [ 57.,   0.,   0.,   1.,   0.,   1.],\n         [ 71.,  11., -23.,  11.,   3., -21.],\n         [ 66.,   1.,   2.,  -1.,   1.,   0.],\n         [ 56.,  -1.,  -2.,   1.,  -1.,   1.],\n         [ 88.,   9.,  -5.,   3.,  -5., -19.],\n         [ 58.,   4.,   1.,   0.,   0.,   0.],\n         [ 63.,  -4.,   0.,   0.,   0.,   0.],\n         [ 64.,  18., -21.,  -2.,  -1.,   0.],\n         [ 54.,  -2.,   2.,   0.,   1.,   0.],\n         [ 63.,   1.,  -1.,   0.,   0.,   0.],\n         [ 72.,  10.,  44.,  11., -10.,   2.],\n         [ 62.,   1.,  -2.,   0.,   0.,   0.],\n         [ 60.,   0.,   1.,   0.,   1.,   0.],\n         [ 82.,  -1.,  26.,  10.,   7.,  -6.],\n         [ 61.,   0.,   0.,   0.,   0.,   0.],\n         [ 60.,   0.,   0.,   0.,   0.,   0.],\n         [ 75.,  11.,  25.,  -6.,   3., -13.],\n         [ 57.,   4.,  -1.,   0.,   1.,  -1.],\n         [ 63.,  -4.,   0.,   0.,   0.,   1.],\n         [ 79.,  -4.,  12., -15.,  10.,   5.],\n         [ 50.,   0.,  -1.,   2.,  -1.,   0.],\n         [ 66.,   0.,   1.,  -1.,   0.,  -1.],\n         [ 60.,  12.,   0.,  -2.,  -5.,   8.],\n         [ 62.,  -1.,   2.,   0.,   1.,   0.],\n         [ 61.,   0.,  -1.,   0.,   0.,   0.],\n         [ 55.,  26.,   3.,  -3.,  -2., -12.],\n         [ 62.,  -1.,   1.,   0.,   0.,   0.],\n         [ 60.,   1.,   0.,   0.,   0.,  -1.],\n         [ 30.,  -1.,   7.,  -8., -10.,   3.],\n         [ 63.,   0.,   1.,  -1.,  -1.,   0.],\n         [ 59.,  -1.,   0.,   0.,   0.,   0.],\n         [ 36.,  -5.,  -8.,   2.,   5.,  13.],\n         [ 62.,   0.,   0.,   0.,   0.,   1.],\n         [ 61.,   0.,   0.,   0.,   1.,   0.]]])"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "output = net(img)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([2, 48, 248])"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [],
   "source": [
    "total_elements = 3 * 4 * 2 * 2  # equals 48\n",
    "\n",
    "# Create a tensor from 1 to total_elements (48 in this case)\n",
    "input_patches = torch.arange(1, total_elements + 1, dtype=torch.float).view(3, 4, 2, 2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[[[ 1.,  2.],\n          [ 3.,  4.]],\n\n         [[ 5.,  6.],\n          [ 7.,  8.]],\n\n         [[ 9., 10.],\n          [11., 12.]],\n\n         [[13., 14.],\n          [15., 16.]]],\n\n\n        [[[17., 18.],\n          [19., 20.]],\n\n         [[21., 22.],\n          [23., 24.]],\n\n         [[25., 26.],\n          [27., 28.]],\n\n         [[29., 30.],\n          [31., 32.]]],\n\n\n        [[[33., 34.],\n          [35., 36.]],\n\n         [[37., 38.],\n          [39., 40.]],\n\n         [[41., 42.],\n          [43., 44.]],\n\n         [[45., 46.],\n          [47., 48.]]]])"
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_patches"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.,  2.,  3.,  4., 17., 18., 19., 20., 33., 34., 35., 36.],\n",
      "        [ 5.,  6.,  7.,  8., 21., 22., 23., 24., 37., 38., 39., 40.],\n",
      "        [ 9., 10., 11., 12., 25., 26., 27., 28., 41., 42., 43., 44.],\n",
      "        [13., 14., 15., 16., 29., 30., 31., 32., 45., 46., 47., 48.]])\n",
      "torch.Size([4, 12])\n"
     ]
    }
   ],
   "source": [
    "# Example input with 3 channels, 4 patches, each patch is 2x2\n",
    "# Simulated input (randomly generated for illustration)\n",
    "# Assume each \"4\" now stands for different patches, each 2x2 in size\n",
    "\n",
    "# Reshape input to flatten each 2x2 patch into a single vector per patch\n",
    "# Flattening each patch\n",
    "flattened_patches = input_patches.view(3, 4, -1)\n",
    "\n",
    "\n",
    "\n",
    "# Optionally, combine channel and spatial information into one dimension\n",
    "# This would result in a [4, 6] shape (4 patches, each 6 values from 2x2x3)\n",
    "flattened_patches = flattened_patches.permute(1, 0, 2).reshape(4, -1)\n",
    "print(flattened_patches)\n",
    "\n",
    "print(flattened_patches.shape)\n",
    "# Output: torch.Size([4, 12])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "class LinearProjection(nn.Module):\n",
    "    def __init__(self, ac, channels, patch_num, d_model = 248):\n",
    "        super(LinearProjection, self).__init__()\n",
    "        self.dct = ac + 1\n",
    "        self.channels = channels\n",
    "        self.d_model = d_model\n",
    "        self.patch_num = patch_num\n",
    "\n",
    "\n",
    "        self.projection = nn.Linear(self.dct * channels, d_model)\n",
    "\n",
    "    def init_weights(self, init_fn):\n",
    "        init_fn(self.projection.weight)\n",
    "\n",
    "    def forward(self, X):\n",
    "        batch_size = X.shape[:-3]\n",
    "        permutate_dim = (0, 2, 1, 3) if batch_size else (1, 0, 2)\n",
    "\n",
    "        X = X.permute(permutate_dim).reshape(*batch_size, self.patch_num, -1)\n",
    "        return self.projection(X)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "outputs": [],
   "source": [
    "class VisionTransformer(nn.Module):\n",
    "    def __init__(self,\n",
    "                 ac: int,\n",
    "                 channels: int,\n",
    "                 patch_num: int,\n",
    "                 num_classes: int,\n",
    "                 d_model: int = 248,\n",
    "                 nhead: int = 8,\n",
    "                 dim_feedforward: int = 1024,\n",
    "                 dropout: int = 0.1,\n",
    "                 activation = nn.ReLU,\n",
    "                 ntransformers: int = 4,\n",
    "                 add_cls_token: bool = True,\n",
    "                 learnable_positional: bool = True):\n",
    "        super(VisionTransformer, self).__init__()\n",
    "\n",
    "        self.add_cls_token = add_cls_token\n",
    "        self.learnable_positional = learnable_positional\n",
    "        self.activation = activation\n",
    "\n",
    "        self.linear_projection = LinearProjection(ac=ac, channels=channels, patch_num=patch_num, d_model=d_model)\n",
    "\n",
    "        self.cls_token = nn.Parameter(torch.zeros(1, 1, d_model)) if add_cls_token else None\n",
    "\n",
    "        if learnable_positional:\n",
    "            if add_cls_token:\n",
    "                self.positional = nn.Parameter(torch.zeros(1, 1 + patch_num, d_model))\n",
    "            else:\n",
    "                self.positional = nn.Parameter(torch.zeros(1, patch_num, d_model))\n",
    "        else:\n",
    "            self.positional = None\n",
    "\n",
    "\n",
    "    def _concat_cls_token(self, X):\n",
    "        if not self.add_cls_token:\n",
    "            return X\n",
    "\n",
    "        batch_size = X.shape[:-2]\n",
    "        if batch_size:\n",
    "            cls_token = self.cls_token.expand(*batch_size, -1, -1)\n",
    "            return torch.cat((cls_token, X), dim=1)\n",
    "        cls_token = self.cls_token.squeeze(0)\n",
    "        return torch.cat((cls_token, X), dim = 0)\n",
    "\n",
    "\n",
    "    def _with_positional(self, X):\n",
    "        return X + self.positional if self.learnable_positional else X if X.shape[:-2] else X.unsqueeze(0) # prevents transformer encoder from receiving unbatched input\n",
    "\n",
    "\n",
    "    def forward(self, X):\n",
    "        X = self.linear_projection(X)\n",
    "        X = self._concat_cls_token(X)\n",
    "        return self._with_positional(X)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "outputs": [],
   "source": [
    "batched, tgt = next(iter(train_loader))\n",
    "batched = batched.to(torch.float32)\n",
    "unbatched = torch.rand(3, 16, 6)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "outputs": [],
   "source": [
    "vit = VisionTransformer(ac=5,\n",
    "                        channels=3,\n",
    "                        patch_num=16,\n",
    "                        num_classes=10,\n",
    "                        d_model=248,\n",
    "                        nhead=8,\n",
    "                        dim_feedforward=1024,\n",
    "                        dropout=0.1,\n",
    "                        activation=nn.ReLU,\n",
    "                        add_cls_token=False,\n",
    "                        learnable_positional=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([16, 16, 248])"
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vit(batched).shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([16, 248])"
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vit.linear_projection(unbatched).shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([1, 16, 248])"
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vit(unbatched).shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}