{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision.datasets import CIFAR10\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "import os\n",
    "from scipy.fft import dctn, idctn\n",
    "from typing import Optional, Tuple, Any\n",
    "from copy import deepcopy\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "# torch.set_printoptions(threshold=10_000)\n",
    "# np.set_printoptions(threshold=np.inf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "outputs": [],
   "source": [
    "class CIFAR10_custom(CIFAR10):\n",
    "    def __init__(self,*args, **kwargs) -> None:\n",
    "        super(CIFAR10_custom, self).__init__(*args, **kwargs)\n",
    "        self.format = 'rgb'\n",
    "\n",
    "\n",
    "    def to_ycbcr(self, in_place = False) -> Optional[Tuple[np.ndarray, list]]:\n",
    "        r\"\"\"Convert entire dataset from RGB to YCbCr.\n",
    "\n",
    "             Args:\n",
    "                in_place (bool, optional): Whether to modify the entire CIFAR10 dataset in memory.\n",
    "                If set to False, returns the (images, targets) as tuples where images are of\n",
    "                shape (B, C, H, W). Defaults to False.\n",
    "        \"\"\"\n",
    "        assert self.format != 'compressed', f'cannot transform format {self.format} into ycbcr, have you ran to_ycbcr after compressed?'\n",
    "\n",
    "        if self.format == 'ycbcr' and not in_place: # prevent applying transformation twice\n",
    "            return self.data.transpose((0, 3, 1, 2)), self.targets\n",
    "\n",
    "        if self.format == 'ycbcr' and in_place: # prevent applying transformation twice\n",
    "            return None\n",
    "\n",
    "        data = self.data.transpose((0, 3, 1, 2))  # convert to CHW\n",
    "\n",
    "\n",
    "        r = data[..., 0, :, :]\n",
    "        g = data[..., 1, :, :]\n",
    "        b = data[..., 2, :, :]\n",
    "\n",
    "        y = 0.299 * r + 0.587 * g + 0.114 * b\n",
    "        cb = -0.168736 * r - 0.331264 * g + 0.5 * b + 128\n",
    "        cr = 0.5 * r - 0.418688 * g - 0.081312 * b + 128\n",
    "\n",
    "\n",
    "        if in_place:\n",
    "            self.format = 'ycbcr'\n",
    "            self.data = np.stack((y, cb, cr), axis=-3).transpose((0, 2, 3, 1)) # convert to HWC\n",
    "        else:\n",
    "            return np.stack((y, cb, cr), axis=-3), self.targets\n",
    "\n",
    "\n",
    "    def compress(self, in_place = False, *args, **kwargs) -> Optional[Tuple[np.ndarray, list]]:\n",
    "        r\"\"\"Compresses the entire dataset from YCbCr format.\n",
    "\n",
    "        Args:\n",
    "            in_place (bool, optional): Whether to modify the entire CIFAR10 dataset in memory.\n",
    "                If set to False, returns the (images, targets) as tuples where images are of\n",
    "                shape (B, C, H, W). Defaults to False.\n",
    "            block_size (tuple[int, int], optional): Size of the window to apply compression.\n",
    "                Defaults to (8, 8).\n",
    "            alpha (int, optional): Alpha parameter to control the magnitude of compression.\n",
    "                Defaults to 1.\n",
    "        \"\"\"\n",
    "\n",
    "\n",
    "        assert self.format != 'rgb', f'format should be ycbcr but is {self.format}, call .to_ycbcr(in_place = True) before compress'\n",
    "\n",
    "        if self.format == 'compressed' and not in_place: # prevent applying transformation twice\n",
    "            return self.data.transpose((0, 3, 1, 2)), self.targets\n",
    "\n",
    "        if self.format == 'compressed' and in_place: # prevent applying transformation twice\n",
    "            return None\n",
    "\n",
    "        data = self.data.transpose((0, 3, 1, 2)) # convert to CHW\n",
    "\n",
    "        data = apply_across_batch(data, compress_quantise_across_channels, *args, **kwargs)\n",
    "\n",
    "        if in_place:\n",
    "            self.format = 'compressed'\n",
    "            self.data = data.transpose((0, 2, 3, 1)) # convert to HWC\n",
    "        else:\n",
    "            return data, self.targets\n",
    "\n",
    "    def __getitem__(self, index: int) -> Tuple[Any, Any]:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            index (int): Index\n",
    "\n",
    "        Returns:\n",
    "            tuple: (image, target) where target is index of the target class.\n",
    "        \"\"\"\n",
    "\n",
    "        if self.format == 'rgb':\n",
    "            return super().__getitem__(index)\n",
    "\n",
    "        if self.format in ('compressed', 'ycbcr'):\n",
    "\n",
    "            img, target = self.data[index], self.targets[index]\n",
    "\n",
    "            if self.transform is not None:\n",
    "                self._check_transform()\n",
    "                img = self.transform(img)\n",
    "\n",
    "            if self.target_transform is not None:\n",
    "                target = self.target_transform(target)\n",
    "\n",
    "            return img, target\n",
    "\n",
    "    def _check_transform(self) -> Any:\n",
    "        invalid_transform = torchvision.transforms.transforms.__name__\n",
    "\n",
    "        transformations = deepcopy(self.transform.__getstate__()['transforms'])\n",
    "\n",
    "        are_valid = list(map(lambda t: t.__class__.__module__ != invalid_transform, transformations))\n",
    "\n",
    "        assert all(are_valid), f\"base image transformations from torchvision are not supported when format is {self.format}, use custom ones from preprocessing.transforms\"\n",
    "\n",
    "\n",
    "\n",
    "def apply_across_batch(array, func, *args, **kwargs):\n",
    "    # Get the shape of the input array\n",
    "    batch_size, channels, H, W = array.shape\n",
    "\n",
    "    # Initialize an output array of the same shape as the input\n",
    "    output = np.zeros_like(array)\n",
    "\n",
    "    # Iterate over the batch and channel dimensions\n",
    "    for i in range(batch_size):\n",
    "        output[i, :, :, :] = func(array[i, :, :, :], *args, **kwargs)\n",
    "\n",
    "    return output"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "outputs": [],
   "source": [
    "def blockwise_dct(image: np.array, block_size: tuple[int, int] = (8, 8)):\n",
    "    height, width = image.shape\n",
    "    block_height, block_width = block_size\n",
    "\n",
    "    dct_blocks = np.zeros_like(image, dtype=np.float32)\n",
    "\n",
    "\n",
    "    for i in range(0, height, block_height):\n",
    "        for j in range(0, width, block_width):\n",
    "            block = image[i:i+block_height, j:j+block_width]\n",
    "\n",
    "            dct_block = dctn(block, norm='ortho')\n",
    "\n",
    "            dct_blocks[i:i+block_height, j:j+block_width] = dct_block\n",
    "\n",
    "    return dct_blocks"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "outputs": [],
   "source": [
    "def blockwise_quantize(dct: np.ndarray, mode='l', block_size: tuple[int, int] = (8, 8), alpha: int = 1,) -> np.ndarray:\n",
    "    luminance_quantization_matrix = np.array([\n",
    "        [16, 11, 10, 16, 24, 40, 51, 61],\n",
    "        [12, 12, 14, 19, 26, 58, 60, 55],\n",
    "        [14, 13, 16, 24, 40, 57, 69, 56],\n",
    "        [14, 17, 22, 29, 51, 87, 80, 62],\n",
    "        [18, 22, 37, 56, 68, 109, 103, 77],\n",
    "        [24, 35, 55, 64, 81, 104, 113, 92],\n",
    "        [49, 64, 78, 87, 103, 121, 120, 101],\n",
    "        [72, 92, 95, 98, 112, 100, 103, 99]\n",
    "    ])\n",
    "\n",
    "    chrominance_quantization_matrix = np.array([\n",
    "        [17, 18, 24, 47, 99, 99, 99, 99],\n",
    "        [18, 21, 26, 66, 99, 99, 99, 99],\n",
    "        [24, 26, 56, 99, 99, 99, 99, 99],\n",
    "        [47, 66, 99, 99, 99, 99, 99, 99],\n",
    "        [99, 99, 99, 99, 99, 99, 99, 99],\n",
    "        [99, 99, 99, 99, 99, 99, 99, 99],\n",
    "        [99, 99, 99, 99, 99, 99, 99, 99],\n",
    "        [99, 99, 99, 99, 99, 99, 99, 99]\n",
    "    ])\n",
    "\n",
    "\n",
    "    quantization_matrix = luminance_quantization_matrix if mode == 'l' else chrominance_quantization_matrix\n",
    "\n",
    "    quantization_matrix = quantization_matrix * alpha\n",
    "\n",
    "    height, width = dct.shape\n",
    "    block_height_num, block_width_num = height // block_size[0], width // block_size[1]\n",
    "\n",
    "    quantization_matrix_tiled = np.tile(quantization_matrix, (block_height_num, block_width_num))\n",
    "\n",
    "    return np.round(dct / quantization_matrix_tiled).astype(np.int8)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "outputs": [],
   "source": [
    "def compress_quantise_across_channels(image: np.ndarray, block_size: tuple[int, int] = (8, 8), alpha: int = 1, *args, **kwargs):\n",
    "    channels, height, width = image.shape\n",
    "\n",
    "    assert channels == 3, f'channels must be 3 YCbCr but got {channels} instead'\n",
    "\n",
    "\n",
    "    y = blockwise_dct(image[0, :, :], block_size, *args, **kwargs)\n",
    "    cb = blockwise_dct(image[1, :, :], block_size, *args, **kwargs)\n",
    "    cr = blockwise_dct(image[2, :, :],block_size, *args, **kwargs)\n",
    "\n",
    "    y = blockwise_quantize(y, 'l', block_size, alpha, *args, **kwargs)\n",
    "    cb = blockwise_quantize(y, 'c', block_size, alpha, *args, **kwargs)\n",
    "    cr = blockwise_quantize(y, 'c', block_size, alpha, *args, **kwargs)\n",
    "\n",
    "\n",
    "    return np.stack((y, cb, cr), axis=-3)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "outputs": [
    {
     "data": {
      "text/plain": "(3, 32, 32)"
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "array = np.random.rand(3, 32, 32)\n",
    "compress_quantise_across_channels(array).shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "outputs": [],
   "source": [
    "download_path = os.path.join('..', 'data', 'cifar10')\n",
    "\n",
    "if not os.path.exists(download_path):\n",
    "    os.makedirs(download_path)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "\n",
    "cifar = CIFAR10_custom(download_path, transform=transform, download = True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "outputs": [
    {
     "data": {
      "text/plain": "'rgb'"
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cifar.format"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[[0.2314, 0.1686, 0.1961,  ..., 0.6196, 0.5961, 0.5804],\n         [0.0627, 0.0000, 0.0706,  ..., 0.4824, 0.4667, 0.4784],\n         [0.0980, 0.0627, 0.1922,  ..., 0.4627, 0.4706, 0.4275],\n         ...,\n         [0.8157, 0.7882, 0.7765,  ..., 0.6275, 0.2196, 0.2078],\n         [0.7059, 0.6784, 0.7294,  ..., 0.7216, 0.3804, 0.3255],\n         [0.6941, 0.6588, 0.7020,  ..., 0.8471, 0.5922, 0.4824]],\n\n        [[0.2431, 0.1804, 0.1882,  ..., 0.5176, 0.4902, 0.4863],\n         [0.0784, 0.0000, 0.0314,  ..., 0.3451, 0.3255, 0.3412],\n         [0.0941, 0.0275, 0.1059,  ..., 0.3294, 0.3294, 0.2863],\n         ...,\n         [0.6667, 0.6000, 0.6314,  ..., 0.5216, 0.1216, 0.1333],\n         [0.5451, 0.4824, 0.5647,  ..., 0.5804, 0.2431, 0.2078],\n         [0.5647, 0.5059, 0.5569,  ..., 0.7216, 0.4627, 0.3608]],\n\n        [[0.2471, 0.1765, 0.1686,  ..., 0.4235, 0.4000, 0.4039],\n         [0.0784, 0.0000, 0.0000,  ..., 0.2157, 0.1961, 0.2235],\n         [0.0824, 0.0000, 0.0314,  ..., 0.1961, 0.1961, 0.1647],\n         ...,\n         [0.3765, 0.1333, 0.1020,  ..., 0.2745, 0.0275, 0.0784],\n         [0.3765, 0.1647, 0.1176,  ..., 0.3686, 0.1333, 0.1333],\n         [0.4549, 0.3686, 0.3412,  ..., 0.5490, 0.3294, 0.2824]]])"
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cifar[0][0]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "outputs": [],
   "source": [
    "cifar.to_ycbcr(in_place=True)\n",
    "output = cifar.compress(in_place=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "base image transformations from torchvision are not supported when format is compressed, use custom ones from preprocessing.transforms",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mAssertionError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[237], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[43mcifar\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m]\u001B[49m[\u001B[38;5;241m0\u001B[39m]\n",
      "Cell \u001B[0;32mIn[226], line 91\u001B[0m, in \u001B[0;36mCIFAR10_custom.__getitem__\u001B[0;34m(self, index)\u001B[0m\n\u001B[1;32m     88\u001B[0m img, target \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdata[index], \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtargets[index]\n\u001B[1;32m     90\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtransform \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m---> 91\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_check_transform\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     92\u001B[0m     img \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtransform(img)\n\u001B[1;32m     94\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtarget_transform \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "Cell \u001B[0;32mIn[226], line 106\u001B[0m, in \u001B[0;36mCIFAR10_custom._check_transform\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    102\u001B[0m transformations \u001B[38;5;241m=\u001B[39m deepcopy(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtransform\u001B[38;5;241m.\u001B[39m__getstate__()[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtransforms\u001B[39m\u001B[38;5;124m'\u001B[39m])\n\u001B[1;32m    104\u001B[0m are_valid \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlist\u001B[39m(\u001B[38;5;28mmap\u001B[39m(\u001B[38;5;28;01mlambda\u001B[39;00m t: t\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__class__\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__module__\u001B[39m \u001B[38;5;241m!=\u001B[39m invalid_transform, transformations))\n\u001B[0;32m--> 106\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28mall\u001B[39m(are_valid), \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mbase image transformations from torchvision are not supported when format is \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mformat\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m, use custom ones from preprocessing.transforms\u001B[39m\u001B[38;5;124m\"\u001B[39m\n",
      "\u001B[0;31mAssertionError\u001B[0m: base image transformations from torchvision are not supported when format is compressed, use custom ones from preprocessing.transforms"
     ]
    }
   ],
   "source": [
    "cifar[0][0]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "outputs": [],
   "source": [
    "images, _ = output"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[[[ 37., -13.,  -4., ...,   0.,   0.,   0.],\n         [ -7.,  -7.,   2., ...,   0.,   0.,   0.],\n         [  3.,   1.,   3., ...,   0.,   0.,   0.],\n         ...,\n         [ -1.,   0.,   0., ...,   0.,   0.,   0.],\n         [  0.,   0.,   0., ...,   0.,   0.,   0.],\n         [  0.,   0.,   0., ...,   0.,   0.,   0.]],\n\n        [[  2.,  -1.,   0., ...,   0.,   0.,   0.],\n         [  0.,   0.,   0., ...,   0.,   0.,   0.],\n         [  0.,   0.,   0., ...,   0.,   0.,   0.],\n         ...,\n         [  0.,   0.,   0., ...,   0.,   0.,   0.],\n         [  0.,   0.,   0., ...,   0.,   0.,   0.],\n         [  0.,   0.,   0., ...,   0.,   0.,   0.]],\n\n        [[  2.,  -1.,   0., ...,   0.,   0.,   0.],\n         [  0.,   0.,   0., ...,   0.,   0.,   0.],\n         [  0.,   0.,   0., ...,   0.,   0.,   0.],\n         ...,\n         [  0.,   0.,   0., ...,   0.,   0.,   0.],\n         [  0.,   0.,   0., ...,   0.,   0.,   0.],\n         [  0.,   0.,   0., ...,   0.,   0.,   0.]]],\n\n\n       [[[ 71.,  -3.,   5., ...,   0.,   0.,   0.],\n         [ 13., -12.,   4., ...,   0.,   0.,   0.],\n         [-11.,   7.,   4., ...,   0.,   0.,   0.],\n         ...,\n         [ -5.,   1.,   1., ...,   0.,   0.,   0.],\n         [  0.,   0.,   0., ...,   0.,   0.,   0.],\n         [  0.,   0.,   0., ...,   0.,   0.,   0.]],\n\n        [[  4.,   0.,   0., ...,   0.,   0.,   0.],\n         [  1.,  -1.,   0., ...,   0.,   0.,   0.],\n         [  0.,   0.,   0., ...,   0.,   0.,   0.],\n         ...,\n         [  0.,   0.,   0., ...,   0.,   0.,   0.],\n         [  0.,   0.,   0., ...,   0.,   0.,   0.],\n         [  0.,   0.,   0., ...,   0.,   0.,   0.]],\n\n        [[  4.,   0.,   0., ...,   0.,   0.,   0.],\n         [  1.,  -1.,   0., ...,   0.,   0.,   0.],\n         [  0.,   0.,   0., ...,   0.,   0.,   0.],\n         ...,\n         [  0.,   0.,   0., ...,   0.,   0.,   0.],\n         [  0.,   0.,   0., ...,   0.,   0.,   0.],\n         [  0.,   0.,   0., ...,   0.,   0.,   0.]]],\n\n\n       [[[127.,   0.,   0., ...,   0.,   0.,   0.],\n         [  0.,   0.,   0., ...,   0.,   0.,   0.],\n         [  0.,   0.,   0., ...,   0.,   0.,   0.],\n         ...,\n         [  0.,   0.,   0., ...,   0.,   0.,   0.],\n         [  0.,   0.,   0., ...,   0.,   0.,   0.],\n         [  0.,   0.,   0., ...,   0.,   0.,   0.]],\n\n        [[  7.,   0.,   0., ...,   0.,   0.,   0.],\n         [  0.,   0.,   0., ...,   0.,   0.,   0.],\n         [  0.,   0.,   0., ...,   0.,   0.,   0.],\n         ...,\n         [  0.,   0.,   0., ...,   0.,   0.,   0.],\n         [  0.,   0.,   0., ...,   0.,   0.,   0.],\n         [  0.,   0.,   0., ...,   0.,   0.,   0.]],\n\n        [[  7.,   0.,   0., ...,   0.,   0.,   0.],\n         [  0.,   0.,   0., ...,   0.,   0.,   0.],\n         [  0.,   0.,   0., ...,   0.,   0.,   0.],\n         ...,\n         [  0.,   0.,   0., ...,   0.,   0.,   0.],\n         [  0.,   0.,   0., ...,   0.,   0.,   0.],\n         [  0.,   0.,   0., ...,   0.,   0.,   0.]]],\n\n\n       [[[ 28.,  -3.,   0., ...,   0.,   0.,   0.],\n         [-11.,   1.,  -1., ...,   0.,   0.,   0.],\n         [ -3.,   0.,   0., ...,   0.,   0.,   0.],\n         ...,\n         [  0.,   0.,   0., ...,   0.,   0.,   0.],\n         [  0.,   0.,   0., ...,   0.,   0.,   0.],\n         [  0.,   0.,   0., ...,   0.,   0.,   0.]],\n\n        [[  2.,   0.,   0., ...,   0.,   0.,   0.],\n         [ -1.,   0.,   0., ...,   0.,   0.,   0.],\n         [  0.,   0.,   0., ...,   0.,   0.,   0.],\n         ...,\n         [  0.,   0.,   0., ...,   0.,   0.,   0.],\n         [  0.,   0.,   0., ...,   0.,   0.,   0.],\n         [  0.,   0.,   0., ...,   0.,   0.,   0.]],\n\n        [[  2.,   0.,   0., ...,   0.,   0.,   0.],\n         [ -1.,   0.,   0., ...,   0.,   0.,   0.],\n         [  0.,   0.,   0., ...,   0.,   0.,   0.],\n         ...,\n         [  0.,   0.,   0., ...,   0.,   0.,   0.],\n         [  0.,   0.,   0., ...,   0.,   0.,   0.],\n         [  0.,   0.,   0., ...,   0.,   0.,   0.]]]])"
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fimage = images[:4]\n",
    "fimage"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "outputs": [],
   "source": [
    "train_transformations = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.ColorJitter(brightness=0.2),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(0.5, 0.5),\n",
    "])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "outputs": [
    {
     "data": {
      "text/plain": "torchvision.transforms.transforms.ToTensor"
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_transformations.__getstate__()['transforms'][3].__class__"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "outputs": [
    {
     "data": {
      "text/plain": "ToTensor()"
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_transformations.__getstate__()['transforms'][3].__class__()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "outputs": [
    {
     "data": {
      "text/plain": "False"
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torchvision.transforms.transforms.ToTensor in train_transformations.__getstate__()['transforms']"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "outputs": [
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torchvision.transforms.transforms.__name__ is train_transformations.__getstate__()['transforms'][3].__class__.__module__"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}