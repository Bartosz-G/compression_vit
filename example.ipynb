{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-06-01T17:03:17.676253Z",
     "start_time": "2024-06-01T17:03:17.622358Z"
    }
   },
   "source": [
    "import os\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from scipy.fft import dctn, idctn\n",
    "\n",
    "import mlflow\n",
    "import mlflow.pytorch\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision.datasets import CIFAR10\n",
    "from torchvision.transforms import Compose, ToTensor, Normalize\n",
    "from torchvision.models import resnet18\n",
    "from torchvision.models.vision_transformer import VisionTransformer\n",
    "from transformers import get_cosine_with_hard_restarts_schedule_with_warmup\n",
    "\n",
    "\n",
    "from preprocessing.transforms import CompressedToTensor, ZigZagOrder, ChooseAC, FlattenZigZag, ConvertToFrequencyDomain, ConvertToYcbcr, Quantize, ScaledPixels, LUMINANCE_QUANTIZATION_MATRIX, CHROMINANCE_QUANTIZATION_MATRIX\n",
    "from model.init import init_kaiming_normal, set_seed, resume_from_checkpoint, init_truncated_normal\n",
    "from model.vit import CompressedVisionTransformer\n",
    "from model.baseline import ResNet18"
   ],
   "outputs": [],
   "execution_count": 29
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 1. Training parameters init:",
   "id": "bff5ada040bed9b5"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 1.1 Variables init",
   "id": "154a960541e4807"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-01T17:03:17.921474Z",
     "start_time": "2024-06-01T17:03:17.915909Z"
    }
   },
   "cell_type": "code",
   "source": [
    "DOWNLAOD_PATH = os.path.join('data', 'cifar10')\n",
    "SEED = 42\n",
    "VALIDATION_SET = 0.1\n",
    "BATCH_SIZE = 128\n",
    "AC = 5\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ],
   "id": "ad18146e091b5c14",
   "outputs": [],
   "execution_count": 30
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 1.2 Preprocessing steps:",
   "id": "973461dcfe47040d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-01T17:03:18.130861Z",
     "start_time": "2024-06-01T17:03:18.125505Z"
    }
   },
   "cell_type": "code",
   "source": [
    "quantization_matrices = [LUMINANCE_QUANTIZATION_MATRIX, CHROMINANCE_QUANTIZATION_MATRIX, CHROMINANCE_QUANTIZATION_MATRIX]\n",
    "\n",
    "if_you_want_to_get_an_RGB_image = Compose([\n",
    "    ToTensor()\n",
    "    # Returns pixels in range [0-1]\n",
    "])\n",
    "\n",
    "if_you_want_to_get_ycbcr_image = Compose([\n",
    "    CompressedToTensor(), # 3x32x32\n",
    "    # Returns pixels in range [0-255]\n",
    "    ConvertToYcbcr(), # 3x32x32\n",
    "    # Returns pixels in range [0-255]\n",
    "    ScaledPixels()\n",
    "    # Returns pixels in range [0-1]\n",
    "])\n",
    "\n",
    "\n",
    "transform = Compose([\n",
    "    CompressedToTensor(), # 3x32x32\n",
    "    # Returns pixels in range [0-255]\n",
    "    ConvertToYcbcr(), # 3x32x32\n",
    "    # Returns pixels in range [0-255]\n",
    "    ConvertToFrequencyDomain(norm='ortho'), # 3x32x32\n",
    "    Quantize(quantization_matrices=quantization_matrices, alpha=1.0, floor=True), # 3x32x32\n",
    "    ZigZagOrder(), # 3x16x64\n",
    "    ChooseAC(AC), # 3x16x(AC+1)\n",
    "    FlattenZigZag() # 16x(3x(AC+1))\n",
    "])"
   ],
   "id": "ed6eac04e65c0b13",
   "outputs": [],
   "execution_count": 31
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 1.3 Model init:",
   "id": "861c7f761119c7dd"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-01T17:03:18.413068Z",
     "start_time": "2024-06-01T17:03:18.339220Z"
    }
   },
   "cell_type": "code",
   "source": [
    "MODEL_PARAMETERS = {\n",
    "    'image_size': 32,\n",
    "    'patch_size': AC, # for CompressedVisionTransformer set as AC, otherwise set to 8\n",
    "    'num_layers': 4,\n",
    "    'num_heads': 8,\n",
    "    'hidden_dim': 248,\n",
    "    'mlp_dim': 1024,\n",
    "    'dropout': 0.1,\n",
    "    'attention_dropout': 0.1,\n",
    "    'num_classes':10\n",
    "}\n",
    "\n",
    "\n",
    "with set_seed(SEED): # Initialization of parameters now happens at __init__\n",
    "    # model = LinVisionTransformer(**MODEL_PARAMETERS).to(DEVICE)\n",
    "    # model = VisionTransformer(**MODEL_PARAMETERS).to(DEVICE)\n",
    "    model = CompressedVisionTransformer(**MODEL_PARAMETERS).to(DEVICE)"
   ],
   "id": "2048ca28a7194cbe",
   "outputs": [],
   "execution_count": 32
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 1.4 Training init:",
   "id": "45588bc6d648794a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-01T17:03:18.832675Z",
     "start_time": "2024-06-01T17:03:18.818848Z"
    }
   },
   "cell_type": "code",
   "source": [
    "experiment_name = f'vit_full_frequency'\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "lr = 1e-4\n",
    "num_epochs = 10\n",
    "weight_decay = 0.01\n",
    "optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "\n",
    "checkpoint_every_nth_epoch = 1 # Set None for no checkpointing\n",
    "start_epoch = 0\n",
    "checkpoint_folder = 'checkpoints'\n",
    "\n",
    "gradient_clipping = nn.utils.clip_grad_norm_\n",
    "max_grad_norm = 1.0\n",
    "warmup_steps = 1760 # 5 epochs\n",
    "scheduler = get_cosine_with_hard_restarts_schedule_with_warmup(optimizer, \n",
    "        num_warmup_steps=warmup_steps, \n",
    "        num_training_steps=num_epochs * 352)\n",
    "\n",
    "os.makedirs(checkpoint_folder, exist_ok=True)\n",
    "checkpoint_path = os.path.join(checkpoint_folder, experiment_name)"
   ],
   "id": "3b54cdfdf4e32d5",
   "outputs": [],
   "execution_count": 33
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 1.5 Resuming from a checkpoint",
   "id": "f7e0953a818707da"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-01T17:03:19.023678Z",
     "start_time": "2024-06-01T17:03:19.019868Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# load_checkpoint_path = 'change to .pth file path'\n",
    "# model, optimizer, start_epoch = resume_from_checkpoint(checkpoint_path=load_checkpoint_path, model=model, optimizer=optimizer)"
   ],
   "id": "72f949c0e13465d4",
   "outputs": [],
   "execution_count": 34
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 1.6 Collecting all parameters for logging:",
   "id": "f64f2dd7fe41130"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-01T17:03:19.624727Z",
     "start_time": "2024-06-01T17:03:19.612770Z"
    }
   },
   "cell_type": "code",
   "source": [
    "PREPROCESSING_PARAMETERS = {\n",
    "    f'step_{i}': type(t).__name__ for i, t in enumerate(transform.transforms)\n",
    "}\n",
    "\n",
    "TRAINING_PARAMETERS = {\n",
    "    \"criterion\":type(criterion).__name__,\n",
    "    \"optimizer_type\": type(optimizer).__name__,\n",
    "    \"seed\":SEED,\n",
    "    \"batch_size\":BATCH_SIZE,\n",
    "    \"validation\":VALIDATION_SET,\n",
    "    \"gradient_clipping\": max_grad_norm,\n",
    "    \"scheduler\": type(scheduler).__name__,\n",
    "    \"warmup_steps\": warmup_steps,\n",
    "    **optimizer.defaults\n",
    "}\n",
    "\n",
    "MODEL_PARAMETERS = {\n",
    "    \"model\": type(model).__name__,\n",
    "    **MODEL_PARAMETERS\n",
    "}"
   ],
   "id": "c95eb4dc9b5238bc",
   "outputs": [],
   "execution_count": 35
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 2. Extract, Transform and Load",
   "id": "bac2f2c9f25102e5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-01T17:03:21.026225Z",
     "start_time": "2024-06-01T17:03:19.653955Z"
    }
   },
   "cell_type": "code",
   "source": [
    "cifar = CIFAR10(root=DOWNLAOD_PATH, train=True, transform=transform, target_transform=None, download = False)\n",
    "cifar_test = CIFAR10(root=DOWNLAOD_PATH, train=False, transform=transform, target_transform=None, download = False)"
   ],
   "id": "f5f4bac3251a44f6",
   "outputs": [],
   "execution_count": 36
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-01T17:03:21.037194Z",
     "start_time": "2024-06-01T17:03:21.028384Z"
    }
   },
   "cell_type": "code",
   "source": [
    "with set_seed(SEED): # For reproducible results run any random operations with set_seed()\n",
    "    num_train = len(cifar)\n",
    "    num_val = int(0.1 * num_train)\n",
    "    num_train -= num_val\n",
    "\n",
    "    cifar_train, cifar_val = random_split(cifar, [num_train, num_val])"
   ],
   "id": "3dc0d39523bc9225",
   "outputs": [],
   "execution_count": 37
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-01T17:03:21.057770Z",
     "start_time": "2024-06-01T17:03:21.039392Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train = DataLoader(cifar_train, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val = DataLoader(cifar_val, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test = DataLoader(cifar_test, batch_size=cifar_test.__len__(), shuffle=False)"
   ],
   "id": "7c00df673219726e",
   "outputs": [],
   "execution_count": 38
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 3. Parameter tracking for MLFlow",
   "id": "a659e00c190aa597"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "mlflow.set_experiment(experiment_name)\n",
    "with mlflow.start_run():\n",
    "    mlflow.log_param(\"preprocessing_steps\", json.dumps(PREPROCESSING_PARAMETERS))\n",
    "    mlflow.log_param(\"training_parameters\", json.dumps(TRAINING_PARAMETERS))\n",
    "    mlflow.log_param(\"model_parameters\", json.dumps(MODEL_PARAMETERS))\n",
    "\n",
    "\n",
    "    with set_seed(SEED):\n",
    "        for epoch in range(start_epoch, num_epochs):\n",
    "            model.train()\n",
    "            train_loss = 0.0\n",
    "            for images, labels in tqdm(train):\n",
    "                images, labels = images.to(torch.float32).to(DEVICE), labels.to(DEVICE)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                gradient_clipping(model.parameters(), max_grad_norm)\n",
    "                optimizer.step()\n",
    "                scheduler.step()\n",
    "\n",
    "                train_loss += loss.detach().cpu().item() * images.size(0)\n",
    "\n",
    "            train_loss /= len(train.dataset)\n",
    "\n",
    "            mlflow.log_metric(\"train_loss\", train_loss, step=epoch)\n",
    "\n",
    "            model.eval()\n",
    "            val_loss = 0.0\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            total_top2 = 0\n",
    "\n",
    "            with torch.no_grad():\n",
    "                for images, labels in val:\n",
    "                    images, labels = images.to(torch.float32).to(DEVICE), labels.to(DEVICE)\n",
    "\n",
    "                    outputs = model(images)\n",
    "                    loss = criterion(outputs, labels)\n",
    "\n",
    "                    val_loss += loss.item() * images.size(0)\n",
    "                    _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "                    top2_pred = outputs.topk(2, dim=1).indices\n",
    "\n",
    "                    total += labels.size(0)\n",
    "                    correct += (predicted == labels).sum().item()\n",
    "\n",
    "                    correct_top2 = 0\n",
    "                    for i in range(labels.size(0)):\n",
    "                        if labels[i] in top2_pred[i]:\n",
    "                            correct_top2 += 1\n",
    "\n",
    "                    total_top2 += correct_top2\n",
    "\n",
    "            val_loss /= len(val.dataset)\n",
    "            val_accuracy = correct / total\n",
    "            top2_accuracy = total_top2 / total\n",
    "\n",
    "            mlflow.log_metric(\"val_loss\", val_loss, step=epoch)\n",
    "            mlflow.log_metric(\"val_accuracy\", val_accuracy, step=epoch)\n",
    "            mlflow.log_metric(\"val_top2accuracy\", top2_accuracy, step=epoch)\n",
    "\n",
    "            print(f'Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}, Top 2 Validation Accuracy: {top2_accuracy:.4f}')\n",
    "\n",
    "            if checkpoint_every_nth_epoch:\n",
    "                if (epoch + 1) % checkpoint_every_nth_epoch == 0:\n",
    "                    save_path = f'{checkpoint_path}_epoch_{epoch + 1}.pth'\n",
    "                    torch.save({\n",
    "                        'epoch': epoch,\n",
    "                        'model_state_dict': model.state_dict(),\n",
    "                        'optimizer_state_dict': optimizer.state_dict(),\n",
    "                    }, save_path)\n",
    "                    print(f'Checkpoint saved to {save_path}')\n",
    "        else:\n",
    "            save_path = f'{checkpoint_path}_final.pth'\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "            }, save_path)\n",
    "            # mlflow.log_artifact(save_path)\n",
    "            pass\n",
    "            #TODO: Add more metrics"
   ],
   "id": "d253adb5e9e5465c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 4. Saving results:",
   "id": "74d752b3ca00d33a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import shutil\n",
    "import os\n",
    "\n",
    "shutil.make_archive('saved_results', 'zip', 'saved_results')"
   ],
   "id": "4c12ff6f9774fabd"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import os\n",
    "import boto3\n",
    "from botocore.exceptions import NoCredentialsError\n",
    "\n",
    "# Initialize the S3 client\n",
    "s3 = boto3.client('s3')\n",
    "\n",
    "# Define your bucket name and the folder path\n",
    "bucket_name = '#########'\n",
    "folder_path = 'saved_results'\n",
    "s3_folder_path = 'run_one/saved_results'  # The path in S3 where the folder will be uploaded\n",
    "\n",
    "\n",
    "\n",
    "def upload_directory_to_s3(folder_path, bucket_name, s3_folder_path):\n",
    "    for root, dirs, files in os.walk(folder_path):\n",
    "        for file in files:\n",
    "            local_path = os.path.join(root, file)\n",
    "            relative_path = os.path.relpath(local_path, folder_path)\n",
    "            s3_path = os.path.join(s3_folder_path, relative_path).replace(\"\\\\\", \"/\")\n",
    "\n",
    "            try:\n",
    "                s3.upload_file(local_path, bucket_name, s3_path)\n",
    "                print(f'Successfully uploaded {local_path} to s3://{bucket_name}/{s3_path}')\n",
    "            except FileNotFoundError:\n",
    "                print(f'The file {local_path} was not found')\n",
    "            except NoCredentialsError:\n",
    "                print('Credentials not available')\n",
    "\n",
    "# Upload the folder\n",
    "upload_directory_to_s3(folder_path, bucket_name, s3_folder_path)"
   ],
   "id": "f7f529fe2685c6bb"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
